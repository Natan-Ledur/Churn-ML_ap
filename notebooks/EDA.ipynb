{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77257c29",
   "metadata": {},
   "source": [
    "# Projeto de Portfólio: Churn Prediction End-to-End\n",
    "\n",
    "Este notebook demonstra um fluxo completo de ciência de dados para classificação de churn com boas práticas de ML: preparação, validação de dados, EDA, pipelines, validação cruzada, tuning, interpretabilidade e serving via API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a9b8e",
   "metadata": {},
   "source": [
    "## 1) Importações e configuração de ambiente\n",
    "\n",
    "Vamos importar bibliotecas, fixar seeds e inspecionar versões para reprodutibilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random, pathlib, warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score, log_loss, f1_score\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "import optuna\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "import pandera as pa\n",
    "from pandera import Column, Check\n",
    "\n",
    "# Fix seeds para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "print(\"Versions:\")\n",
    "print(\"python:\", sys.version)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"optuna:\", optuna.__version__)\n",
    "print(\"shap:\", shap.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c563e7",
   "metadata": {},
   "source": [
    "## 2) Download e carregamento do dataset público (Telco Churn)\n",
    "\n",
    "Vamos tentar baixar um CSV público do Telco Customer Churn. Caso não haja internet, usamos `data/sample_churn.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PUBLIC_URLS = [\n",
    "    # IBM Telco Churn (espelhos públicos; podem cair)\n",
    "    \"https://raw.githubusercontent.com/blastchar/telco-churn/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\",\n",
    "]\n",
    "\n",
    "fallback = Path(\"../data/sample_churn.csv\")\n",
    "\n",
    "for url in PUBLIC_URLS:\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        src = url\n",
    "        break\n",
    "    except Exception as e:\n",
    "        df = None\n",
    "\n",
    "if df is None:\n",
    "    print(\"Sem internet ou URL indisponível. Usando fallback sample_churn.csv\")\n",
    "    df = pd.read_csv(fallback)\n",
    "    src = str(fallback)\n",
    "\n",
    "print(\"Fonte:\", src)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1c568",
   "metadata": {},
   "source": [
    "## 3) Limpeza e tipagem de dados\n",
    "\n",
    "Normaliza `Churn` em 0/1, converte numéricos e remove colunas irrelevantes como `customerID` (quando existir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()\n",
    "\n",
    "# Normaliza target\n",
    "if 'Churn' in df.columns:\n",
    "    df['Churn'] = df['Churn'].replace({'Yes': 1, 'No': 0, True: 1, False: 0}).astype(int)\n",
    "elif 'churn' in df.columns:\n",
    "    df['churn'] = df['churn'].replace({'Yes': 1, 'No': 0, True: 1, False: 0}).astype(int)\n",
    "\n",
    "# Remove ID\n",
    "for id_col in ['customerID', 'CustomerID', 'customer_id', 'id']:\n",
    "    if id_col in df.columns:\n",
    "        df = df.drop(columns=[id_col])\n",
    "\n",
    "# Converte numéricos que vieram como string\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        # tenta forçar para número se possível\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(df.dtypes.head(20))\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cde94c",
   "metadata": {},
   "source": [
    "## 4) Validação de dados com pandera\n",
    "\n",
    "Definimos um esquema simples para validar colunas básicas (adaptável ao dataset disponível)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf872ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera import Column, Check\n",
    "\n",
    "# Define schema mínimo\n",
    "columns = list(df.columns)\n",
    "num_cols = [c for c in columns if pd.api.types.is_numeric_dtype(df[c]) and c != 'Churn']\n",
    "cat_cols = [c for c in columns if c not in num_cols + ['Churn']]\n",
    "\n",
    "schema = pa.DataFrameSchema({\n",
    "    **{c: Column(float, nullable=True) for c in num_cols},\n",
    "    **{c: Column(object, nullable=True) for c in cat_cols},\n",
    "})\n",
    "\n",
    "_ = schema.validate(df.drop(columns=[c for c in ['Churn','churn'] if c in df.columns], errors='ignore'), lazy=True)\n",
    "print(\"Schema válido para features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377c350",
   "metadata": {},
   "source": [
    "## 5) EDA rápida\n",
    "\n",
    "Distribuição do target, contagens por categóricas e correlações numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Churn' if 'Churn' in df.columns else 'churn'\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "df[target].value_counts(normalize=True).plot(kind='bar', ax=ax[0], title='Distribuição do target')\n",
    "ax[0].set_xlabel('Classe'); ax[0].set_ylabel('Proporção')\n",
    "\n",
    "# Uma categoria exemplo, se existir\n",
    "any_cat = next((c for c in df.columns if df[c].dtype == 'object'), None)\n",
    "if any_cat:\n",
    "    df.groupby([any_cat, target]).size().unstack(fill_value=0).plot(kind='bar', stacked=True, ax=ax[1], title=f'{any_cat} x {target}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr = df.select_dtypes(include=[np.number]).corr(numeric_only=True)\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.title('Correlação numérica')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4ac92",
   "metadata": {},
   "source": [
    "## 6) Split estratificado: treino/val/teste (60/20/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=SEED, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "print({\"train\": len(X_train), \"val\": len(X_val), \"test\": len(X_test)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd34ac",
   "metadata": {},
   "source": [
    "## 7) Engenharia de atributos simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8169d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simple_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    # Exemplo: se 'MonthlyCharges' e 'tenure' estiverem presentes, cria ARPU-like\n",
    "    if set(['MonthlyCharges','tenure']).issubset(df.columns):\n",
    "        df['charges_per_tenure'] = df['MonthlyCharges'] / (df['tenure'].replace(0, np.nan))\n",
    "        df['charges_per_tenure'] = df['charges_per_tenure'].fillna(0)\n",
    "    # Bins simples de tenure\n",
    "    if 'tenure' in df.columns:\n",
    "        df['tenure_bin'] = pd.cut(df['tenure'], bins=[-1, 12, 24, 48, 72, np.inf], labels=['<=12','13-24','25-48','49-72','>72'])\n",
    "    return df\n",
    "\n",
    "X_train_fe = add_simple_features(X_train)\n",
    "X_val_fe = add_simple_features(X_val)\n",
    "X_test_fe = add_simple_features(X_test)\n",
    "\n",
    "print(X_train_fe.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048202f",
   "metadata": {},
   "source": [
    "## 8) Pré-processamento (ColumnTransformer) e 9) Balanceamento com SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd892fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessor(train_df: pd.DataFrame):\n",
    "    cats = [c for c in train_df.columns if train_df[c].dtype == 'object']\n",
    "    nums = [c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c])]\n",
    "\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer([\n",
    "        (\"cat\", cat_pipe, cats),\n",
    "        (\"num\", num_pipe, nums),\n",
    "    ])\n",
    "\n",
    "preprocessor = make_preprocessor(X_train_fe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613d547",
   "metadata": {},
   "source": [
    "## 10) Baseline com DummyClassifier e métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_proba(y_true, y_proba):\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "    ll = log_loss(y_true, np.vstack([1-y_proba, y_proba]).T)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return {\"roc_auc\": auc, \"ap\": ap, \"log_loss\": ll, \"f1\": f1}\n",
    "\n",
    "baseline = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"clf\", DummyClassifier(strategy='most_frequent'))\n",
    "])\n",
    "baseline.fit(X_train_fe, y_train)\n",
    "\n",
    "try:\n",
    "    p_val = baseline.predict_proba(X_val_fe)[:,1]\n",
    "except Exception:\n",
    "    p_val = np.zeros_like(y_val, dtype=float)\n",
    "\n",
    "metrics_baseline = evaluate_proba(y_val.values, p_val)\n",
    "metrics_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35000383",
   "metadata": {},
   "source": [
    "## 11-12) Modelos: LogisticRegression, RandomForest (+ XGBoost se disponível) e CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df54a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(clf):\n",
    "    return Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    \"logreg\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=SEED),\n",
    "    \"rf\": RandomForestClassifier(n_estimators=300, random_state=SEED, class_weight='balanced')\n",
    "}\n",
    "if XGB_AVAILABLE:\n",
    "    models[\"xgb\"] = XGBClassifier(\n",
    "        n_estimators=300, learning_rate=0.1, subsample=0.9, colsample_bytree=0.8,\n",
    "        max_depth=5, random_state=SEED, eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "cv_results = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = make_pipeline(clf)\n",
    "    scores = cross_val_score(pipe, X_train_fe, y_train, cv=cv, scoring='roc_auc')\n",
    "    cv_results[name] = {\"roc_auc_mean\": scores.mean(), \"roc_auc_std\": scores.std()}\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3ad45",
   "metadata": {},
   "source": [
    "## 13) Tuning com Optuna (exemplo rápido)\n",
    "\n",
    "Atenção: pode ser demorado. Aqui um estudo pequeno só para demonstrar o fluxo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    C = trial.suggest_float('C', 1e-3, 10.0, log=True)\n",
    "    clf = LogisticRegression(max_iter=1000, class_weight='balanced', C=C, random_state=SEED)\n",
    "    pipe = make_pipeline(clf)\n",
    "    scores = cross_val_score(pipe, X_train_fe, y_train, cv=cv, scoring='roc_auc')\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=False)\n",
    "study.best_params, study.best_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7cd38",
   "metadata": {},
   "source": [
    "## 14) Avaliação no teste e curvas ROC/PR + matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = study.best_params.get('C', 1.0)\n",
    "best_pipe = make_pipeline(LogisticRegression(max_iter=1000, class_weight='balanced', C=best_C, random_state=SEED))\n",
    "best_pipe.fit(pd.concat([X_train_fe, X_val_fe]), pd.concat([y_train, y_val]))\n",
    "\n",
    "proba_test = best_pipe.predict_proba(X_test_fe)[:,1]\n",
    "metrics_test = evaluate_proba(y_test.values, proba_test)\n",
    "metrics_test\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, proba_test)\n",
    "prec, rec, _ = precision_recall_curve(y_test, proba_test)\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(fpr, tpr); ax[0].set_title('ROC'); ax[0].set_xlabel('FPR'); ax[0].set_ylabel('TPR')\n",
    "ax[1].plot(rec, prec); ax[1].set_title('Precision-Recall'); ax[1].set_xlabel('Recall'); ax[1].set_ylabel('Precision')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, (proba_test>=0.5).astype(int), normalize='true')\n",
    "sns.heatmap(cm, annot=True, cmap='Blues'); plt.title('Matriz de Confusão (norm)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5c9a5",
   "metadata": {},
   "source": [
    "## 15) Interpretabilidade: Permutation Importance e SHAP (se disponível)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525bf8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "r = permutation_importance(best_pipe, X_val_fe, y_val, scoring='roc_auc', n_repeats=5, random_state=SEED)\n",
    "# Como estamos em um Pipeline com OHE, nomes das features expandidas não estão triviais; mostramos top importâncias genéricas\n",
    "imp = pd.Series(r.importances_mean).sort_values(ascending=False).head(20)\n",
    "imp.plot(kind='bar', title='Permutation Importance (top 20)')\n",
    "plt.show()\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    try:\n",
    "        explainer = shap.Explainer(best_pipe.named_steps['clf'])\n",
    "        X_trans = best_pipe.named_steps['pre'].fit_transform(X_train_fe)\n",
    "        shap_values = explainer(X_trans[:200])\n",
    "        shap.plots.beeswarm(shap_values)\n",
    "    except Exception as e:\n",
    "        print(\"SHAP plot skip:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98a6e5",
   "metadata": {},
   "source": [
    "## 16) Persistência do pipeline e metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "ART_DIR = Path('../models'); ART_DIR.mkdir(exist_ok=True, parents=True)\n",
    "MODEL_PATH = ART_DIR / 'best_pipeline.joblib'\n",
    "META_PATH = ART_DIR / 'metadata.json'\n",
    "\n",
    "joblib.dump(best_pipe, MODEL_PATH)\n",
    "\n",
    "# Tenta obter hash do git, se for repositório\n",
    "try:\n",
    "    commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()\n",
    "except Exception:\n",
    "    commit = None\n",
    "\n",
    "meta = {\n",
    "    'created_at': datetime.utcnow().isoformat(),\n",
    "    'metrics_test': metrics_test,\n",
    "    'lib_versions': {\n",
    "        'numpy': np.__version__, 'pandas': pd.__version__, 'sklearn': sklearn.__version__,\n",
    "        'optuna': optuna.__version__, 'shap': shap.__version__\n",
    "    },\n",
    "    'git_commit': commit,\n",
    "}\n",
    "with open(META_PATH, 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "MODEL_PATH, META_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937dd86",
   "metadata": {},
   "source": [
    "## 17) Função de inferência e contrato de I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9114505",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_infer = pa.DataFrameSchema({\n",
    "    **{c: Column(float, nullable=True) for c in X_test_fe.select_dtypes(include=np.number).columns},\n",
    "    **{c: Column(object, nullable=True) for c in X_test_fe.select_dtypes(exclude=np.number).columns},\n",
    "})\n",
    "\n",
    "def predict_proba_batch(df_in: pd.DataFrame):\n",
    "    _ = schema_infer.validate(df_in, lazy=True)\n",
    "    proba = best_pipe.predict_proba(df_in)[:,1]\n",
    "    return pd.DataFrame({\"proba\": proba, \"pred\": (proba>=0.5).astype(int)})\n",
    "\n",
    "predict_proba_batch(X_test_fe.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d7282",
   "metadata": {},
   "source": [
    "## 18) API mínima com FastAPI para servir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    records: list[dict]\n",
    "\n",
    "@app.post('/predict')\n",
    "def predict_api(payload: Payload):\n",
    "    df_in = pd.DataFrame(payload.records)\n",
    "    df_in = add_simple_features(df_in)\n",
    "    out = predict_proba_batch(df_in)\n",
    "    return {\"predictions\": out.to_dict(orient='records')}\n",
    "\n",
    "print(\"Para rodar: uvicorn EDA:app --reload --port 8000 (no diretório deste notebook)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352eedc6",
   "metadata": {},
   "source": [
    "## 19) Testes automatizados (asserts) no notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ace9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica que superamos baseline em ROC AUC no teste por um delta mínimo\n",
    "assert metrics_test[\"roc_auc\"] >= metrics_baseline.get(\"roc_auc\", 0) + 1e-6\n",
    "\n",
    "# Verifica contrato de I/O\n",
    "out = predict_proba_batch(X_test_fe.head(3))\n",
    "assert set(out.columns) == {\"proba\", \"pred\"}\n",
    "assert len(out) == 3\n",
    "\n",
    "print(\"Notebooks checks OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c42e0d",
   "metadata": {},
   "source": [
    "## 20) Reprodutibilidade e ambiente\n",
    "\n",
    "Registramos `pip freeze` para um arquivo de artifacts e reimprimimos versões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_ENV = Path('../models'); ART_ENV.mkdir(parents=True, exist_ok=True)\n",
    "req_path = ART_ENV / 'requirements.txt'\n",
    "\n",
    "# Grava freeze\n",
    "try:\n",
    "    import subprocess\n",
    "    freeze = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']).decode()\n",
    "    with open(req_path, 'w') as f:\n",
    "        f.write(freeze)\n",
    "    print('Requisitos salvos em', req_path)\n",
    "except Exception as e:\n",
    "    print('Falha ao capturar pip freeze:', e)\n",
    "\n",
    "print(\"Versions again:\")\n",
    "print(\"numpy:\", np.__version__, \"pandas:\", pd.__version__, \"sklearn:\", sklearn.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
